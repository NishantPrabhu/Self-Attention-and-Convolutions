
# Configuration for Bert Self attention

epochs: 301
batch_size: 100
eval_every: 10

# Models
bert_encoder:
  name: relative
  mha_heads: 9
  num_encoder_blocks: 6
  model_dim: 400
  ff_dim: 512 
  pre_norm: False
  use_attention_data: False
  query_positional_score: True
  max_position_embedding: 32
  position_encoding_size: -1
  attention_dropout_prob: 0.1

conv_bottom:
  name: resnet50
  block: 1
  pretrained: False
  model_dim: 400
  pool_with_resnet: False
  pool_concatenate_size: 2

clf_head:
  model_dim: 400
  n_classes: 10

optimizer:
  name: sgd
  lr: 0.1
  weight_decay: 1.e-04

scheduler:
  name: cosine
  warmup_epochs: 15

# Data
dataset:

  name: cifar10
  root: ../data/cifar10
  
  train_transform:
    random_flip:
    random_crop:
      size: 32
      padding: 4
    to_tensor:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std: [0.2023, 0.1994, 0.2010]

  val_transform:
    to_tensor:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std: [0.2023, 0.1994, 0.2010]
