
# Configuration for Bert Self attention

epochs: 300
batch_size: 100
eval_every: 10

# Models
bert_encoder:
  name: relative
  mha_heads: 9
  num_encoder_blocks: 6
  in_dim: 256
  model_dim: 400
  ff_dim: 512 
  pre_norm: False
  use_attention_data: True
  query_positional_score: True
  max_position_embedding: 16
  position_encoding_size: -1
  attention_dropout_prob: 0.1

conv_bottom:
  name: resnet50
  block: 1
  pretrained: False

clf_head:
  in_dim: 256
  n_classes: 10

optimizer:
  name: sgd
  lr: 0.1
  weight_decay: 1.e-04

scheduler:
  name: cosine
  warmup_epochs: 15

# Data
dataset:

  name: cifar10
  root: ../data/cifar10
  
  train_transform:
    to_tensor:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std: [0.2470, 0.2435, 0.2616]

  val_transform:
    to_tensor:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std: [0.2470, 0.2435, 0.2616]
