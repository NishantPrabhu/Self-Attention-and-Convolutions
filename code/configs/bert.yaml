
# Configuration for Bert Self attention

epochs: 300
batch_size: 100
eval_every: 5

# Models
bert_encoder:
  name: bert
  mha_heads: 9
  num_encoder_blocks: 6
  model_dim: 400
  ff_dim: 512
  pre_norm: False

conv_bottom:
  name: resnet50
  block: 1
  pretrained: False
  model_dim: 400
  pool_with_resnet: False 
  pool_concatenate_size: 4

clf_head:
  model_dim: 400
  n_classes: 10

optimizer:
  name: sgd
  lr: 0.1
  weight_decay: 1.e-04

scheduler:
  name: cosine
  warmup_epochs: 15

# Data
dataset:

  name: cifar10
  root: ../data/cifar10
  
  train_transform:
    random_flip:
    # gaussian_blur:
    #   sigma: [0.1, 2.0]
    #   apply_prob: 0.5
    # random_resized_crop:
    #   size: 32
    #   scale: [0.2, 1.0]
    to_tensor:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std: [0.2470, 0.2435, 0.2616]

  val_transform:
    random_flip:
    to_tensor:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std: [0.2470, 0.2435, 0.2616]
