batch_size: 100
bert_encoder:
  attention: gaussian
  attention_dropout_prob: 0.1
  attention_gaussian_blur_trick: false
  attention_isotropic_gaussian: false
  ff_activation: relu
  ff_dim: 512
  gaussian_mu_init_noise: 2.0
  gaussian_sigma_init_noise: 0.01
  model_dim: 400
  num_encoder_blocks: 6
  num_heads: 9
  pre_norm: false
clf_head:
  model_dim: 400
  num_classes: 10
dataset:
  name: cifar10
  root: data/cifar10
  train_transform:
    normalize:
      mean:
      - 0.4914
      - 0.4822
      - 0.4465
      std:
      - 0.2023
      - 0.1994
      - 0.201
    random_crop:
      padding: 4
      size: 32
    random_flip: null
    to_tensor: null
  val_transform:
    normalize:
      mean:
      - 0.4914
      - 0.4822
      - 0.4465
      std:
      - 0.2023
      - 0.1994
      - 0.201
    to_tensor: null
epochs: 300
eval_every: 10
feature_pooling:
  block: 1
  model_dim: 400
  pool_downsample_size: 2
  pool_with_resnet: false
  pretrained: false
  resnet: resnet50
optimizer:
  lr: 0.1
  name: sgd
  weight_decay: 0.0001
scheduler:
  name: cosine
  warmup_epochs: 15
